{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ***Bonus exercise: advanced grade***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "from utils import emojis_unicode, emoticons, slang_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lowercase(text):\n",
    "    out = text.lower()\n",
    "    print(f\"After lowercase: {out}\")\n",
    "    return out\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    PUNCT_TO_REMOVE = string.punctuation\n",
    "    out = text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))\n",
    "    print(f\"After removing punctuation: {out}\")\n",
    "    return out\n",
    "\n",
    "def stopwords_removal(text):\n",
    "    STOPWORDS = set(stopwords.words('english'))\n",
    "    out = \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
    "    print(f\"After stopwords removal: {out}\")\n",
    "    return out\n",
    "\n",
    "def lemmatize_words(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    wordnet_map = {\"N\":wordnet.NOUN, \"V\":wordnet.VERB, \"J\":wordnet.ADJ, \"R\":wordnet.ADV}\n",
    "    pos_tagged_text = nltk.pos_tag(text.split())\n",
    "    out = \" \".join([lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_tagged_text])\n",
    "    print(f\"After lematizing: {out}\")\n",
    "    return out\n",
    "\n",
    "def convert_emojis(text):\n",
    "    EMO_UNICODE = emojis_unicode()\n",
    "    UNICODE_EMO = {v: k for k, v in EMO_UNICODE.items()}\n",
    "    print(text)\n",
    "    for emoticon, description in UNICODE_EMO.items():\n",
    "        cleaned_description = description.replace(\",\", \"\").replace(\":\", \"\").split()\n",
    "        replacement = \"_\".join(cleaned_description)\n",
    "        text = text.replace(emoticon, replacement)\n",
    "    print(f\"After converting emojis: {text}\")\n",
    "    return text\n",
    "\n",
    "def convert_emoticons(text):\n",
    "    EMOTICONS = emoticons()\n",
    "    for emoticon, description in EMOTICONS.items():\n",
    "        cleaned_description = description.replace(\",\", \"\").split()\n",
    "        cleaned_description_joined = \"_\".join(cleaned_description)\n",
    "        # replace the emojis by the cleaned description within the given text\n",
    "        out = re.sub(u'('+emoticon+')', cleaned_description_joined, text)\n",
    "    print(f\"After converting emoticons: {out}\")\n",
    "    return out\n",
    "\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    out = url_pattern.sub(r'', text)\n",
    "    print(f\"After removing urls: {out}\")\n",
    "    return out\n",
    "\n",
    "def remove_html(text):\n",
    "    html_pattern = re.compile('<.*?>')\n",
    "    out = html_pattern.sub(r'', text)\n",
    "    print(f\"After removing html: {out}\")\n",
    "    return out\n",
    "\n",
    "def chat_words_conversion(text):\n",
    "    slang_words_list = slang_words()\n",
    "    chat_words_list = list(slang_words_list.keys())\n",
    "    new_text = []\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words_list:\n",
    "            new_text.append(slang_words_list[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    out = \" \".join(new_text)\n",
    "    print(f\"After converting slang: {out}\")\n",
    "    return out \n",
    "\n",
    "# Create FunctionTransformer for each custom function\n",
    "lowercase_transformer = FunctionTransformer(func=lowercase, validate=False)\n",
    "remove_punctuation_transformer = FunctionTransformer(func=remove_punctuation, validate=False)\n",
    "stopwords_removal_transformer = FunctionTransformer(func=stopwords_removal, validate=False)\n",
    "lemmatize_words_transformer = FunctionTransformer(func=lemmatize_words, validate=False)\n",
    "convert_emojis_transformer = FunctionTransformer(func=convert_emojis, validate=False)\n",
    "convert_emoticons_transformer = FunctionTransformer(func=convert_emoticons, validate=False)\n",
    "remove_urls_transformer = FunctionTransformer(func=remove_urls, validate=False)\n",
    "remove_html_transformer = FunctionTransformer(func=remove_html, validate=False)\n",
    "chat_words_conversion_transformer = FunctionTransformer(func=chat_words_conversion, validate=False)\n",
    "\n",
    "# Create an scikit-learn pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('remove_urls', remove_urls_transformer),\n",
    "    ('remove_html', remove_html_transformer),\n",
    "    ('convert_emojis', convert_emojis_transformer),\n",
    "    ('convert_emoticons', convert_emoticons_transformer),\n",
    "    ('chat_words_conversion', chat_words_conversion_transformer),\n",
    "    ('lowercase', lowercase_transformer),\n",
    "    ('remove_punctuation', remove_punctuation_transformer),\n",
    "    ('stopwords_removal', stopwords_removal_transformer),\n",
    "    ('lemmatize_words', lemmatize_words_transformer),\n",
    "    \n",
    "])\n",
    "\n",
    "def clean(text, pipeline=pipeline):\n",
    "    return pipeline.transform(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing urls: Hello Amazon - my package never arrived :(  PLEASE FIX ASAP ‚è∞! @AmazonHelp <test/>\n",
      "After removing html: Hello Amazon - my package never arrived :(  PLEASE FIX ASAP ‚è∞! @AmazonHelp \n",
      "Hello Amazon - my package never arrived :(  PLEASE FIX ASAP ‚è∞! @AmazonHelp \n",
      "After converting emojis: Hello Amazon - my package never arrived :(  PLEASE FIX ASAP alarm_clock! @AmazonHelp \n",
      "After converting emoticons: Hello Amazon - my package never arrived :(  PLEASE FIX ASAP alarm_clock! @AmazonHelp \n",
      "After converting slang: Hello Amazon - my package never arrived :( PLEASE FIX As Soon As Possible alarm_clock! @AmazonHelp\n",
      "After lowercase: hello amazon - my package never arrived :( please fix as soon as possible alarm_clock! @amazonhelp\n",
      "After removing punctuation: hello amazon  my package never arrived  please fix as soon as possible alarmclock amazonhelp\n",
      "After stopwords removal: hello amazon package never arrived please fix soon possible alarmclock amazonhelp\n",
      "After lematizing: hello amazon package never arrive please fix soon possible alarmclock amazonhelp\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'hello amazon package never arrive please fix soon possible alarmclock amazonhelp'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.transform(\"Hello Amazon - my package never arrived :( https://www.amazon.com/gp/css/order-history?ref_=nav_orders_first PLEASE FIX ASAP ‚è∞! @AmazonHelp <test/>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>index</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hello Amazon - my package never arrived :( htt...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hello! üòä This is an example text with emojis! üëç</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;p&gt;This is a &lt;b&gt;sample&lt;/b&gt; text with &lt;a href='...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The quick brown fox jumps over the lazy dog.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Visit our website at https://www.example.com f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I'm feeling üòÑ today. Don't worry üòâ.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>This text contains special characters #$%&amp;@*!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LOL BRB and OMG are common chat abbreviations.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>üòÇüòçüëè Just saw the funniest movie ever! üòÇüòçüëè</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>&lt;a href='https://www.example.com'&gt;Click here&lt;/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I found a great recipe at https://www.recipes....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>M8 imho this NLP thing is kinda üî• !</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Hate Is A Place Where A Man Who Can‚Äôt Stand Sa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>This Sword Is The Proof That I Have Lived.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text\n",
       "index                                                   \n",
       "1      Hello Amazon - my package never arrived :( htt...\n",
       "2        Hello! üòä This is an example text with emojis! üëç\n",
       "3      <p>This is a <b>sample</b> text with <a href='...\n",
       "4           The quick brown fox jumps over the lazy dog.\n",
       "5      Visit our website at https://www.example.com f...\n",
       "6                    I'm feeling üòÑ today. Don't worry üòâ.\n",
       "7          This text contains special characters #$%&@*!\n",
       "8         LOL BRB and OMG are common chat abbreviations.\n",
       "9              üòÇüòçüëè Just saw the funniest movie ever! üòÇüòçüëè\n",
       "10     <a href='https://www.example.com'>Click here</...\n",
       "11     I found a great recipe at https://www.recipes....\n",
       "12                   M8 imho this NLP thing is kinda üî• !\n",
       "13     Hate Is A Place Where A Man Who Can‚Äôt Stand Sa...\n",
       "14            This Sword Is The Proof That I Have Lived."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"./to_clean.csv\", index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "df[\"cleaned_text\"] = df.text.apply(lambda x: clean(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base text: Hello Amazon - my package never arrived :( https://www.amazon.com/gp/css/order-history?ref_=nav_orders_first PLEASE FIX ASAP ‚è∞! @AmazonHelp <test/>\n",
      "Cleaned text: hello amazon package never arrive please fix soon possible alarmclock amazonhelp\n",
      "\n",
      "\n",
      "Base text: Hello! üòä This is an example text with emojis! üëç\n",
      "Cleaned text: hello smilingfacewithsmilingeyes example text emojis thumbsup\n",
      "\n",
      "\n",
      "Base text: <p>This is a <b>sample</b> text with <a href='https://www.example.com'>HTML</a> tags.</p>\n",
      "Cleaned text: sample text\n",
      "\n",
      "\n",
      "Base text: The quick brown fox jumps over the lazy dog.\n",
      "Cleaned text: quick brown fox jump lazy dog\n",
      "\n",
      "\n",
      "Base text: Visit our website at https://www.example.com for more information\n",
      "Cleaned text: visit website information\n",
      "\n",
      "\n",
      "Base text: I'm feeling üòÑ today. Don't worry üòâ.\n",
      "Cleaned text: im feel smilingfacewithopenmouthsmilingeyes today dont worry winkingface\n",
      "\n",
      "\n",
      "Base text: This text contains special characters #$%&@*!\n",
      "Cleaned text: text contain special character\n",
      "\n",
      "\n",
      "Base text: LOL BRB and OMG are common chat abbreviations.\n",
      "Cleaned text: laugh loud right back omg common chat abbreviation\n",
      "\n",
      "\n",
      "Base text: üòÇüòçüëè Just saw the funniest movie ever! üòÇüòçüëè\n",
      "Cleaned text: facewithtearsofjoysmilingfacewithhearteyesclappinghands saw funny movie ever facewithtearsofjoysmilingfacewithhearteyesclappinghands\n",
      "\n",
      "\n",
      "Base text: <a href='https://www.example.com'>Click here</a> for more info\n",
      "Cleaned text: info\n",
      "\n",
      "\n",
      "Base text: I found a great recipe at https://www.recipes.com! üòã It's so delicious! #cooking\n",
      "Cleaned text: find great recipe facesavouringdeliciousfood delicious cooking\n",
      "\n",
      "\n",
      "Base text: M8 imho this NLP thing is kinda üî• !\n",
      "Cleaned text: mate humble opinion nlp thing kinda fire\n",
      "\n",
      "\n",
      "Base text: Hate Is A Place Where A Man Who Can‚Äôt Stand Sadness Goes. üòû\n",
      "Cleaned text: hate place man can‚Äôt stand sadness go disappointedface\n",
      "\n",
      "\n",
      "Base text: This Sword Is The Proof That I Have Lived.\n",
      "Cleaned text: sword proof live\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, row in df.iterrows():\n",
    "    print(f\"Base text: {row.text}\")\n",
    "    print(f\"Cleaned text: {row.cleaned_text}\")\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-courses-4P5UDvBG-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
