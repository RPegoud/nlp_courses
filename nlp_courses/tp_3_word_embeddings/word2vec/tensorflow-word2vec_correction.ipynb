{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade pip\n",
    "!pip install tensorflow numpy==1.16 sklearn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_raw = '''\n",
    "He is the king . \n",
    "The king is royal . \n",
    "She is the royal queen . \n",
    "The queen and the king are royal . \n",
    "The prince is the son of the king . \n",
    "The princess is the daughter of the queen . \n",
    "Royal families live in palaces . \n",
    "Palaces have many rooms . \n",
    "The prince and the princess are young .\n",
    "'''\n",
    "corpus_raw = corpus_raw.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = []\n",
    "\n",
    "# parse through the corpus and remove the dots\n",
    "for word in corpus_raw.split():\n",
    "    if word != '.': \n",
    "        words.append(word)\n",
    "\n",
    "# Convert words into integers and vice versa\n",
    "word2int = {}\n",
    "int2word = {}\n",
    "vocab_size = len(words)\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "    # TODO 1: Fill in the logic for word2int and int2word mappings\n",
    "    # for example: word_to_int has words as keys and integers as value \n",
    "    word2int[word] = i\n",
    "    int2word[i] = word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw sentences is a list of sentences.\n",
    "raw_sentences = corpus_raw.split('.')\n",
    "sentences = []\n",
    "for sentence in raw_sentences:\n",
    "    sentences.append(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 2\n",
    "data = []\n",
    "for sentence in sentences:\n",
    "    for word_index, word in enumerate(sentence):\n",
    "        for nb_word in sentence[max(word_index - WINDOW_SIZE, 0) : min(word_index + WINDOW_SIZE, len(sentence)) + 1] : \n",
    "            if nb_word != word:\n",
    "                data.append([word, nb_word])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***What does the previous cell do ?***\n",
    "* ***Your** thoughts here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def to_one_hot(data_point_index, vocab_size):\n",
    "    # TODO 2: Implement the one-hot encoding function here.\n",
    "    # initialize a list of zeros equal to vocab_size\n",
    "    # turn this list into a one hot vector for the data_point_index\n",
    "    temp = np.zeros(vocab_size)\n",
    "    temp[data_point_index] = 1\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x_train = [] # input words\n",
    "y_train = [] # output words\n",
    "\n",
    "for data_word in data:\n",
    "    # TODO 3: Append appropriate one-hot encoded vectors to x_train and y_train\n",
    "    x_train.append(to_one_hot(word2int[ data_word[0] ], vocab_size))\n",
    "    y_train.append(to_one_hot(word2int[ data_word[1] ], vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert lists to numpy arrays\n",
    "x_train = np.asarray(x_train)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=(None, vocab_size))\n",
    "y_label = tf.placeholder(tf.float32, shape=(None, vocab_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***What does the previous cell do ?***\n",
    "* ***Your** thoughts here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "EMBEDDING_DIM = 5\n",
    "\n",
    "# TODO 5: Initialize weight matrices W1 and W2 with random values, and biases b1 and b2.\n",
    "# Use tf.Variable and tf.random_normal with the appropriate shapes to initialize layers and biases\n",
    "# look at the slides if you don't remember the layer shapes \n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([vocab_size, EMBEDDING_DIM]))\n",
    "b1 = tf.Variable(tf.random_normal([EMBEDDING_DIM]))\n",
    "W2 = tf.Variable(tf.random_normal([EMBEDDING_DIM, vocab_size]))\n",
    "b2 = tf.Variable(tf.random_normal([vocab_size]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO 6: Calculate the hidden representation of the word.\n",
    "# \"hidden representation\" means the value of the vector after the first layer\n",
    "# i.e. the vector within the latent space of the network\n",
    "# use tf.add and tf.matmul\n",
    "hidden_representation = tf.add(tf.matmul(x,W1), b1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# TODO 7: Compute the prediction, the output from the output layer.\n",
    "# hint: the prediction follows is the hidden representation passed through the second layer\n",
    "# don't forget to add the softmax to turn the outputs into a probability distribution\n",
    "# you may use tf.nn.softmax for that \n",
    "prediction = tf.nn.softmax(tf.add( tf.matmul(hidden_representation, W2), b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***What does the previous cell do ?***\n",
    "* ***Your** thoughts here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function:\n",
    "loss = tf.reduce_mean(-tf.reduce_sum(y_label * tf.log(prediction), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Which loss function is this ?***\n",
    "* ***Your** thoughts here*\n",
    "\n",
    "### ***Why do we use it here ?***\n",
    "* ***Your** thoughts here*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the training step:\n",
    "# TODO 10: Use an optimizer to minimize the loss.\n",
    "# look through the documentation:\n",
    "# https://www.tensorflow.org/api_docs/python/tf/compat/v1/train/GradientDescentOptimizer\n",
    "# tf.compat.v1 is replaced with tf in our code\n",
    "train_step = tf.train.GradientDescentOptimizer(0.1).minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_iters = 10000\n",
    "\n",
    "for _ in range(n_iters):\n",
    "    sess.run(train_step, feed_dict={x: x_train, y_label: y_train})\n",
    "\n",
    "vectors = sess.run(W1 + b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Reminder***\n",
    "\n",
    "Euclidian distance:\n",
    "$$ ||x-y||_2 = \\sqrt{ \\sum_{i=1}^{N} (x_i - y_i)^2 } $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO 11 (last one!): implement the euclidian distance function using numpy\n",
    "def euclidean_dist(vec1, vec2):\n",
    "    return np.sqrt(np.sum((vec1-vec2)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***What is TSNE ? How is it useful here ?***\n",
    "* ***Your** thoughts here*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "model = TSNE(n_components=2, perplexity=1, random_state=0)\n",
    "np.set_printoptions(suppress=True)\n",
    "vectors = model.fit_transform(vectors) \n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "normalizer = preprocessing.Normalizer()\n",
    "vectors =  normalizer.fit_transform(vectors, 'l2')\n",
    "\n",
    "print(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "print(words)\n",
    "for word in words:\n",
    "    print(word, vectors[word2int[word]][1])\n",
    "    ax.annotate(word, (vectors[word2int[word]][0],vectors[word2int[word]][1] ))\n",
    "    ax.set_xlim(min([vectors[word2int[w]][0] for w in words])-1, max([vectors[word2int[w]][0] for w in words])+1)\n",
    "    ax.set_ylim(min([vectors[word2int[w]][1] for w in words])-1, max([vectors[word2int[w]][1] for w in words])+1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***You should see something similar (you can rerun the two last cells) to improve the results:***\n",
    "<img src=\"word_2_vec_result.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ***Comment on what you've understood from this course***\n",
    "For example:\n",
    "* What is word2vec ?\n",
    "* Why is it an improvement on BoW and TF-IDF\n",
    "* What is an encoder network, can you imagine an example to illustrate this notion ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
